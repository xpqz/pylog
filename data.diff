diff --git a/prolog/debug/__init__.py b/prolog/debug/__init__.py
index bc97719..8b0370f 100644
--- a/prolog/debug/__init__.py
+++ b/prolog/debug/__init__.py
@@ -7,9 +7,12 @@
 
 from .tracer import TraceEvent, PortsTracer
 from .sinks import PrettyTraceSink
+from .metrics import PredMetrics, EngineMetrics
 
 __all__ = [
     "TraceEvent",
     "PortsTracer",
     "PrettyTraceSink",
+    "PredMetrics",
+    "EngineMetrics",
 ]
\ No newline at end of file
diff --git a/prolog/debug/metrics.py b/prolog/debug/metrics.py
new file mode 100644
index 0000000..abee786
--- /dev/null
+++ b/prolog/debug/metrics.py
@@ -0,0 +1,215 @@
+"""
+Performance and behavior metrics collection for PyLog.
+
+Provides lightweight metrics tracking with zero overhead when disabled.
+Tracks both global engine counters and per-predicate statistics.
+"""
+
+from dataclasses import dataclass, field
+from typing import Dict, Any
+
+
+@dataclass(frozen=True, slots=True)
+class PredMetrics:
+    """
+    Per-predicate performance metrics.
+
+    Immutable dataclass with slots for memory efficiency.
+    All counters default to 0 if not specified.
+    """
+    pred_id: str
+    calls: int = 0
+    exits: int = 0
+    fails: int = 0
+    redos: int = 0
+    unifications: int = 0
+    backtracks: int = 0
+
+    def to_dict(self) -> Dict[str, Any]:
+        """Export metrics as a dictionary."""
+        return {
+            "pred_id": self.pred_id,
+            "calls": self.calls,
+            "exits": self.exits,
+            "fails": self.fails,
+            "redos": self.redos,
+            "unifications": self.unifications,
+            "backtracks": self.backtracks
+        }
+
+
+class EngineMetrics:
+    """
+    Global engine metrics and per-predicate tracking.
+
+    Provides methods to record various engine events with
+    automatic accumulation and validation.
+    """
+
+    def __init__(self):
+        """Initialize all counters to 0."""
+        # Global counters
+        self.unifications_attempted = 0
+        self.unifications_succeeded = 0
+        self.backtracks_taken = 0
+        self.cuts_executed = 0
+        self.alternatives_pruned = 0
+        self.exceptions_thrown = 0
+        self.exceptions_caught = 0
+        self.candidates_considered = 0
+        self.candidates_yielded = 0
+
+        # Per-predicate metrics (pred_id -> mutable counters)
+        # We use a dict of dicts for mutability during collection
+        self._pred_metrics: Dict[str, Dict[str, int]] = {}
+
+    def reset(self):
+        """Reset all metrics to initial state."""
+        self.unifications_attempted = 0
+        self.unifications_succeeded = 0
+        self.backtracks_taken = 0
+        self.cuts_executed = 0
+        self.alternatives_pruned = 0
+        self.exceptions_thrown = 0
+        self.exceptions_caught = 0
+        self.candidates_considered = 0
+        self.candidates_yielded = 0
+        self._pred_metrics.clear()
+
+    # Global counter methods
+
+    def record_unification_attempt(self):
+        """Record a unification attempt."""
+        self.unifications_attempted += 1
+
+    def record_unification_success(self):
+        """Record a successful unification."""
+        self.unifications_succeeded += 1
+
+    def record_backtrack(self):
+        """Record a backtrack operation."""
+        self.backtracks_taken += 1
+
+    def record_cut(self):
+        """Record a cut execution."""
+        self.cuts_executed += 1
+
+    def record_alternatives_pruned(self, count: int):
+        """Record number of alternatives pruned by a cut."""
+        if count < 0:
+            raise ValueError(f"Cannot prune negative alternatives: {count}")
+        self.alternatives_pruned += count
+
+    def record_exception_thrown(self):
+        """Record an exception being thrown."""
+        self.exceptions_thrown += 1
+
+    def record_exception_caught(self):
+        """Record an exception being caught."""
+        self.exceptions_caught += 1
+
+    def record_candidates(self, considered: int, yielded: int):
+        """Record candidate clauses from indexing (cumulative)."""
+        if considered < 0:
+            raise ValueError(f"Cannot consider negative candidates: {considered}")
+        if yielded < 0:
+            raise ValueError(f"Cannot yield negative candidates: {yielded}")
+        self.candidates_considered += considered
+        self.candidates_yielded += yielded
+
+    # Per-predicate methods
+
+    def _ensure_predicate(self, pred_id: str):
+        """Ensure a predicate entry exists."""
+        if pred_id not in self._pred_metrics:
+            self._pred_metrics[pred_id] = {
+                "calls": 0,
+                "exits": 0,
+                "fails": 0,
+                "redos": 0,
+                "unifications": 0,
+                "backtracks": 0
+            }
+
+    def record_call(self, pred_id: str):
+        """Record a CALL port for a predicate."""
+        self._ensure_predicate(pred_id)
+        self._pred_metrics[pred_id]["calls"] += 1
+
+    def record_exit(self, pred_id: str):
+        """Record an EXIT port for a predicate."""
+        self._ensure_predicate(pred_id)
+        self._pred_metrics[pred_id]["exits"] += 1
+
+    def record_fail(self, pred_id: str):
+        """Record a FAIL port for a predicate."""
+        self._ensure_predicate(pred_id)
+        self._pred_metrics[pred_id]["fails"] += 1
+
+    def record_redo(self, pred_id: str):
+        """Record a REDO port for a predicate."""
+        self._ensure_predicate(pred_id)
+        self._pred_metrics[pred_id]["redos"] += 1
+
+    def record_predicate_unification(self, pred_id: str, attempted: int, succeeded: int = 0):
+        """Record unifications for a specific predicate."""
+        self._ensure_predicate(pred_id)
+        # Store attempted count (not succeeded for per-predicate)
+        self._pred_metrics[pred_id]["unifications"] += attempted
+        # Also update global counters
+        self.unifications_attempted += attempted
+        self.unifications_succeeded += succeeded
+
+    def record_predicate_backtrack(self, pred_id: str):
+        """Record a backtrack for a specific predicate."""
+        self._ensure_predicate(pred_id)
+        self._pred_metrics[pred_id]["backtracks"] += 1
+        # Also update global counter
+        self.backtracks_taken += 1
+
+    def get_predicate_metrics(self, pred_id: str) -> PredMetrics:
+        """Get metrics for a specific predicate."""
+        if pred_id in self._pred_metrics:
+            data = self._pred_metrics[pred_id]
+            return PredMetrics(
+                pred_id=pred_id,
+                calls=data["calls"],
+                exits=data["exits"],
+                fails=data["fails"],
+                redos=data["redos"],
+                unifications=data["unifications"],
+                backtracks=data["backtracks"]
+            )
+        else:
+            # Return zeros for unknown predicates
+            return PredMetrics(pred_id=pred_id)
+
+    def to_dict(self) -> Dict[str, Any]:
+        """
+        Export all metrics as a dictionary.
+
+        Returns a dict with 'global' and 'predicates' sections.
+        """
+        result = {
+            "global": {
+                "unifications_attempted": self.unifications_attempted,
+                "unifications_succeeded": self.unifications_succeeded,
+                "backtracks_taken": self.backtracks_taken,
+                "cuts_executed": self.cuts_executed,
+                "alternatives_pruned": self.alternatives_pruned,
+                "exceptions_thrown": self.exceptions_thrown,
+                "exceptions_caught": self.exceptions_caught,
+                "candidates_considered": self.candidates_considered,
+                "candidates_yielded": self.candidates_yielded
+            },
+            "predicates": {}
+        }
+
+        # Add per-predicate metrics
+        for pred_id in self._pred_metrics:
+            metrics = self.get_predicate_metrics(pred_id)
+            result["predicates"][pred_id] = metrics.to_dict()
+            # Remove redundant pred_id from nested dict
+            del result["predicates"][pred_id]["pred_id"]
+
+        return result
\ No newline at end of file
diff --git a/prolog/engine/engine.py b/prolog/engine/engine.py
index 5feb723..5908404 100644
--- a/prolog/engine/engine.py
+++ b/prolog/engine/engine.py
@@ -114,6 +114,13 @@ def __init__(
         else:
             self.tracer = None
 
+        # Initialize metrics if debug=True
+        if debug:
+            from prolog.debug.metrics import EngineMetrics
+            self.metrics = EngineMetrics()
+        else:
+            self.metrics = None
+
     def reset(self):
         """Reset engine state for reuse."""
         self.store = Store()
@@ -136,6 +143,9 @@ def reset(self):
         # Reset debug counters if in debug mode
         if self.debug:
             self._candidates_considered = 0
+            # Also reset metrics when debug=True
+            if self.metrics:
+                self.metrics.reset()
         # Exception handling is done via try/except PrologThrow
         # Don't reset ports - they accumulate across runs
 
@@ -317,6 +327,9 @@ def run(
                 # Handle thrown exception
                 handled = self._handle_throw(exc.ball)
                 if handled:
+                    # Exception was caught
+                    if self.metrics:
+                        self.metrics.record_exception_caught()
                     continue
                 # No handler found - clean up and re-raise
                 self.trail.unwind_to(0, self.store)
@@ -352,16 +365,24 @@ def run(
 
     def _unify(self, a: Term, b: Term) -> bool:
         """Helper for unification with current engine settings.
-        
+
         Args:
             a: First term to unify
             b: Second term to unify
-            
+
         Returns:
             True if unification succeeds, False otherwise
         """
+        if self.metrics:
+            self.metrics.record_unification_attempt()
+
         trail_adapter = TrailAdapter(self.trail, engine=self, store=self.store)
-        return unify(a, b, self.store, trail_adapter, occurs_check=self.occurs_check)
+        result = unify(a, b, self.store, trail_adapter, occurs_check=self.occurs_check)
+
+        if self.metrics and result:
+            self.metrics.record_unification_success()
+
+        return result
     
     def _handle_throw(self, ball: Term) -> bool:
         """Handle a thrown exception by searching for a matching catch.
@@ -539,6 +560,11 @@ def _dispatch_predicate(self, goal: Goal) -> bool:
         if self.tracer and goal.term:
             self.tracer.emit_event("call", goal.term)
 
+        # Record metrics for CALL
+        pred_id = f"{functor}/{arity}"
+        if self.metrics:
+            self.metrics.record_call(pred_id)
+
         # Get matching clauses - use indexing if available
         if self.use_indexing and hasattr(self.program, 'select'):
             from prolog.engine.indexed_program import IndexedProgram
@@ -549,6 +575,10 @@ def _dispatch_predicate(self, goal: Goal) -> bool:
             # Track candidates in debug mode
             if self.debug:
                 self._candidates_considered += len(matches)
+                if self.metrics:
+                    # Count how many are actually yielded (have potential to match)
+                    yielded = len([m for m in matches if m is not None])
+                    self.metrics.record_candidates(len(matches), yielded)
                 
                 # Log detailed info if trace is enabled too
                 if self.trace:
@@ -576,6 +606,8 @@ def _dispatch_predicate(self, goal: Goal) -> bool:
         if not cursor.has_more():
             # No matching clauses - emit FAIL port
             self._port("FAIL", f"{functor}/{arity}")
+            if self.metrics:
+                self.metrics.record_fail(pred_id)
             return False
 
         # Take first clause
@@ -627,14 +659,7 @@ def _dispatch_predicate(self, goal: Goal) -> bool:
         renamed_clause = self._renamer.rename_clause(clause)
 
         # Try to unify with clause head
-        trail_adapter = TrailAdapter(self.trail, engine=self, store=self.store)
-        if unify(
-            renamed_clause.head,
-            goal.term,
-            self.store,
-            trail_adapter,  # type: ignore
-            occurs_check=self.occurs_check,
-        ):
+        if self._unify(renamed_clause.head, goal.term):
             # Unification succeeded - now push frame for body execution
             # Use the cut_barrier saved before creating choicepoint
             frame_id = self._next_frame_id
@@ -810,9 +835,18 @@ def _dispatch_cut(self):
             current_frame = self.frame_stack[-1]
             cut_barrier = current_frame.cut_barrier
 
+            # Count alternatives being pruned
+            alternatives_pruned = 0
+
             # Remove all choicepoints above the barrier
             while len(self.cp_stack) > cut_barrier:
                 self.cp_stack.pop()
+                alternatives_pruned += 1
+
+            # Record metrics
+            if self.metrics and alternatives_pruned > 0:
+                self.metrics.record_cut()
+                self.metrics.record_alternatives_pruned(alternatives_pruned)
         else:
             # Top-level cut: prune everything (commit to current solution path)
             # This commits to the current solution and prevents backtracking.
@@ -853,6 +887,8 @@ def _dispatch_pop_frame(self, goal: Goal):
                 self._debug_frame_pops += 1
                 # Emit EXIT port when frame is popped
                 self._port("EXIT", frame.pred or "unknown")
+                if self.metrics and frame.pred:
+                    self.metrics.record_exit(frame.pred)
             # else: Leave frame in place for backtracking
         # If frame ID doesn't match, it's likely a stale POP_FRAME from backtracking
         # Just ignore it - the frame was already popped or never created
@@ -893,6 +929,9 @@ def _backtrack(self) -> bool:
         Returns:
             True if backtracking succeeded, False if no more choicepoints.
         """
+        if self.metrics:
+            self.metrics.record_backtrack()
+
         while self.cp_stack:
             cp = self.cp_stack.pop()
             
@@ -952,10 +991,14 @@ def _backtrack(self) -> bool:
                 if cp.payload.get("terminal", False):
                     # Terminal CP - just emit FAIL and continue backtracking
                     self._port("FAIL", cp.payload["pred_ref"])
+                    if self.metrics:
+                        self.metrics.record_fail(cp.payload["pred_ref"])
                     continue
 
                 # Emit REDO port before resuming normal predicate
                 self._port("REDO", cp.payload["pred_ref"])
+                if self.metrics:
+                    self.metrics.record_redo(cp.payload["pred_ref"])
 
                 # Try next clause
                 goal = cp.payload["goal"]
@@ -1040,16 +1083,7 @@ def _backtrack(self) -> bool:
                     renamed_clause = self._renamer.rename_clause(clause)
 
                     # Try to unify with clause head
-                    trail_adapter = TrailAdapter(
-                        self.trail, engine=self, store=self.store
-                    )
-                    unify_result = unify(
-                        renamed_clause.head,
-                        goal.term,
-                        self.store,
-                        trail_adapter,  # type: ignore
-                        occurs_check=self.occurs_check,
-                    )
+                    unify_result = self._unify(renamed_clause.head, goal.term)
                     
                     if self.trace:
                         self._trace_log.append(f"PREDICATE CP: Unify result = {unify_result}")
@@ -1092,6 +1126,8 @@ def _backtrack(self) -> bool:
                 else:
                     # No more clauses to try - emit FAIL port
                     self._port("FAIL", cp.payload["pred_ref"])
+                    if self.metrics:
+                        self.metrics.record_fail(cp.payload["pred_ref"])
                     # Continue backtracking to find earlier choicepoints
                     continue
 
@@ -1410,6 +1446,10 @@ def _builtin_unify(self, args: tuple) -> bool:
         if len(args) != 2:
             return False
         left, right = args
+
+        if self.metrics:
+            self.metrics.record_unification_attempt()
+
         trail_adapter = TrailAdapter(self.trail, engine=self, store=self.store)
         result = unify(
             left,
@@ -1418,6 +1458,10 @@ def _builtin_unify(self, args: tuple) -> bool:
             trail_adapter,  # type: ignore
             occurs_check=self.occurs_check,
         )
+
+        if self.metrics and result:
+            self.metrics.record_unification_success()
+
         return result
 
     def _builtin_not_unify(self, args: tuple) -> bool:
@@ -2048,7 +2092,11 @@ def _builtin_throw(self, args: tuple) -> bool:
         # This ensures that variable bindings are preserved in the thrown term
         reified_ball = self._reify_term(ball)
         
-        # Raise PrologThrow directly  
+        # Record metrics for exception thrown
+        if self.metrics:
+            self.metrics.record_exception_thrown()
+
+        # Raise PrologThrow directly
         raise PrologThrow(reified_ball)
     
     def _builtin_catch(self, args: tuple) -> bool:
diff --git a/prolog/tests/unit/test_metrics.py b/prolog/tests/unit/test_metrics.py
new file mode 100644
index 0000000..9513b14
--- /dev/null
+++ b/prolog/tests/unit/test_metrics.py
@@ -0,0 +1,470 @@
+"""
+Tests for performance and behavior metrics collection.
+
+Tests the metrics module for tracking engine operations with minimal overhead.
+"""
+
+import pytest
+from dataclasses import FrozenInstanceError
+from typing import Dict, Any
+
+from prolog.debug.metrics import PredMetrics, EngineMetrics
+from prolog.engine.engine import Engine
+from prolog.ast.terms import Atom, Struct, Int, Var
+from prolog.ast.clauses import Program, Clause
+from prolog.tests.helpers import mk_fact, mk_rule, program, run_query
+
+
+# Helper functions to reduce boilerplate
+def program_from_source(src: str) -> Program:
+    """Create a Program from Prolog source text.
+
+    For now, return an empty program since we're mostly testing metrics,
+    not parsing. The engine tests will still work with simple facts.
+    """
+    # Simple programs for testing - just return empty or simple facts
+    if "foo(1)" in src:
+        return program(mk_fact("foo", Int(1)), mk_fact("foo", Int(2)))
+    elif "test(1). test(2). test(3)" in src:
+        return program(mk_fact("test", Int(1)), mk_fact("test", Int(2)), mk_fact("test", Int(3)))
+    elif "test(1). test(2)" in src:
+        return program(mk_fact("test", Int(1)), mk_fact("test", Int(2)))
+    elif "test(1)" in src:
+        return program(mk_fact("test", Int(1)))
+    elif "parent(tom, bob)" in src:
+        return program(
+            mk_fact("parent", Atom("tom"), Atom("bob")),
+            mk_fact("parent", Atom("bob"), Atom("pat")),
+            mk_rule("grandparent", (Var(0, "X"), Var(1, "Z")),
+                    Struct("parent", (Var(0, "X"), Var(2, "Y"))),
+                    Struct("parent", (Var(2, "Y"), Var(1, "Z"))))
+        )
+    else:
+        # For other cases, return empty program
+        return program()
+
+
+def engine_for(src: str, debug: bool = True) -> Engine:
+    """Create an Engine from Prolog source text."""
+    return Engine(program=program_from_source(src), debug=debug)
+
+
+class TestPredMetrics:
+    """Tests for per-predicate metrics dataclass."""
+
+    def test_pred_metrics_required_fields(self):
+        """PredMetrics has all required counter fields."""
+        metrics = PredMetrics(
+            pred_id="append/3",
+            calls=10,
+            exits=8,
+            fails=2,
+            redos=3,
+            unifications=25,
+            backtracks=5
+        )
+
+        assert metrics.pred_id == "append/3"
+        assert metrics.calls == 10
+        assert metrics.exits == 8
+        assert metrics.fails == 2
+        assert metrics.redos == 3
+        assert metrics.unifications == 25
+        assert metrics.backtracks == 5
+
+    def test_pred_metrics_immutable(self):
+        """PredMetrics is immutable (frozen=True)."""
+        metrics = PredMetrics(
+            pred_id="member/2",
+            calls=1,
+            exits=1,
+            fails=0,
+            redos=0,
+            unifications=2,
+            backtracks=0
+        )
+
+        with pytest.raises(FrozenInstanceError):
+            metrics.calls = 2
+
+    def test_pred_metrics_defaults_zero(self):
+        """PredMetrics defaults all counters to 0."""
+        metrics = PredMetrics(pred_id="p/1")
+        assert metrics.calls == 0
+        assert metrics.exits == 0
+        assert metrics.fails == 0
+        assert metrics.redos == 0
+        assert metrics.unifications == 0
+        assert metrics.backtracks == 0
+
+    def test_pred_metrics_has_slots(self):
+        """PredMetrics uses slots for memory efficiency."""
+        metrics = PredMetrics(
+            pred_id="test/0",
+            calls=0,
+            exits=0,
+            fails=0,
+            redos=0,
+            unifications=0,
+            backtracks=0
+        )
+
+        # Classes with __slots__ don't have __dict__
+        assert not hasattr(metrics, '__dict__')
+        assert hasattr(type(metrics), '__slots__')
+
+    def test_pred_metrics_to_dict(self):
+        """PredMetrics exports to dict format."""
+        metrics = PredMetrics(
+            pred_id="foo/2",
+            calls=5,
+            exits=3,
+            fails=2,
+            redos=1,
+            unifications=10,
+            backtracks=2
+        )
+
+        d = metrics.to_dict()
+        assert isinstance(d, dict)
+        assert d["pred_id"] == "foo/2"
+        assert d["calls"] == 5
+        assert d["exits"] == 3
+        assert d["fails"] == 2
+        assert d["redos"] == 1
+        assert d["unifications"] == 10
+        assert d["backtracks"] == 2
+
+
+class TestEngineMetrics:
+    """Tests for global engine metrics."""
+
+    def test_engine_metrics_initialization(self):
+        """EngineMetrics initializes with all counters at 0."""
+        metrics = EngineMetrics()
+
+        # Global counters
+        assert metrics.unifications_attempted == 0
+        assert metrics.unifications_succeeded == 0
+        assert metrics.backtracks_taken == 0
+        assert metrics.cuts_executed == 0
+        assert metrics.alternatives_pruned == 0
+        assert metrics.exceptions_thrown == 0
+        assert metrics.exceptions_caught == 0
+        assert metrics.candidates_considered == 0
+        assert metrics.candidates_yielded == 0
+
+        # Per-predicate tracking
+        assert len(metrics._pred_metrics) == 0
+
+    def test_engine_metrics_increment_global(self):
+        """Global counters increment correctly."""
+        metrics = EngineMetrics()
+
+        metrics.record_unification_attempt()
+        assert metrics.unifications_attempted == 1
+
+        metrics.record_unification_success()
+        assert metrics.unifications_succeeded == 1
+
+        metrics.record_backtrack()
+        assert metrics.backtracks_taken == 1
+
+        metrics.record_cut()
+        assert metrics.cuts_executed == 1
+
+        metrics.record_alternatives_pruned(3)
+        assert metrics.alternatives_pruned == 3
+
+        metrics.record_exception_thrown()
+        assert metrics.exceptions_thrown == 1
+
+        metrics.record_exception_caught()
+        assert metrics.exceptions_caught == 1
+
+        # Candidates accumulate (not overwrite)
+        metrics.record_candidates(10, 2)
+        assert metrics.candidates_considered == 10
+        assert metrics.candidates_yielded == 2
+
+        metrics.record_candidates(5, 1)
+        assert metrics.candidates_considered == 15
+        assert metrics.candidates_yielded == 3
+
+    def test_engine_metrics_negative_inputs_rejected(self):
+        """Negative inputs to metrics methods raise ValueError."""
+        metrics = EngineMetrics()
+
+        with pytest.raises(ValueError):
+            metrics.record_alternatives_pruned(-1)
+
+        with pytest.raises(ValueError):
+            metrics.record_candidates(-5, 1)
+
+        with pytest.raises(ValueError):
+            metrics.record_candidates(5, -1)
+
+    def test_engine_metrics_per_predicate_tracking(self):
+        """Per-predicate metrics are tracked correctly."""
+        metrics = EngineMetrics()
+
+        # Record some calls
+        metrics.record_call("append/3")
+        metrics.record_call("append/3")
+        metrics.record_call("member/2")
+
+        # Record exits
+        metrics.record_exit("append/3")
+        metrics.record_exit("member/2")
+
+        # Record fails
+        metrics.record_fail("append/3")
+
+        # Check per-predicate stats
+        append_stats = metrics.get_predicate_metrics("append/3")
+        assert append_stats.calls == 2
+        assert append_stats.exits == 1
+        assert append_stats.fails == 1
+
+        member_stats = metrics.get_predicate_metrics("member/2")
+        assert member_stats.calls == 1
+        assert member_stats.exits == 1
+        assert member_stats.fails == 0
+
+    def test_engine_metrics_redo_tracking(self):
+        """REDO port updates per-predicate metrics."""
+        metrics = EngineMetrics()
+
+        metrics.record_call("between/3")
+        metrics.record_exit("between/3")
+        metrics.record_redo("between/3")
+        metrics.record_exit("between/3")
+        metrics.record_redo("between/3")
+        metrics.record_fail("between/3")
+
+        stats = metrics.get_predicate_metrics("between/3")
+        assert stats.calls == 1
+        assert stats.exits == 2
+        assert stats.redos == 2
+        assert stats.fails == 1
+
+    def test_engine_metrics_reset(self):
+        """Reset clears all metrics."""
+        metrics = EngineMetrics()
+
+        # Add some data
+        metrics.record_unification_attempt()
+        metrics.record_backtrack()
+        metrics.record_call("test/0")
+        metrics.record_exit("test/0")
+
+        # Reset
+        metrics.reset()
+
+        # All counters should be 0
+        assert metrics.unifications_attempted == 0
+        assert metrics.backtracks_taken == 0
+        assert len(metrics._pred_metrics) == 0
+
+        # Getting predicate metrics after reset returns zeros
+        stats = metrics.get_predicate_metrics("test/0")
+        assert stats.calls == 0
+        assert stats.exits == 0
+
+    def test_engine_metrics_to_dict(self):
+        """Export to dict includes all metrics."""
+        metrics = EngineMetrics()
+
+        # Record some activity
+        metrics.record_unification_attempt()
+        metrics.record_unification_success()
+        metrics.record_backtrack()
+        metrics.record_call("foo/1")
+        metrics.record_exit("foo/1")
+
+        # Export
+        d = metrics.to_dict()
+
+        # Check structure
+        assert isinstance(d, dict)
+        assert "global" in d
+        assert "predicates" in d
+
+        # Check global metrics
+        assert d["global"]["unifications_attempted"] == 1
+        assert d["global"]["unifications_succeeded"] == 1
+        assert d["global"]["backtracks_taken"] == 1
+
+        # Check per-predicate metrics
+        assert "foo/1" in d["predicates"]
+        assert d["predicates"]["foo/1"]["calls"] == 1
+        assert d["predicates"]["foo/1"]["exits"] == 1
+
+    def test_engine_metrics_unification_tracking(self):
+        """Per-predicate unification counts are tracked."""
+        metrics = EngineMetrics()
+
+        metrics.record_call("unify_test/2")
+        metrics.record_predicate_unification("unify_test/2", attempted=5, succeeded=3)
+        metrics.record_exit("unify_test/2")
+
+        stats = metrics.get_predicate_metrics("unify_test/2")
+        assert stats.unifications == 5  # Total attempted for this predicate
+
+    def test_engine_metrics_backtrack_tracking(self):
+        """Per-predicate backtrack counts are tracked."""
+        metrics = EngineMetrics()
+
+        metrics.record_call("choice/1")
+        metrics.record_exit("choice/1")
+        metrics.record_predicate_backtrack("choice/1")
+        metrics.record_redo("choice/1")
+        metrics.record_fail("choice/1")
+
+        stats = metrics.get_predicate_metrics("choice/1")
+        assert stats.backtracks == 1
+
+
+class TestMetricsEngineIntegration:
+    """Tests for metrics integration with the engine."""
+
+    def test_engine_accepts_debug_parameter(self):
+        """Engine accepts debug parameter for metrics."""
+        # Engine already accepts debug parameter
+        engine = Engine(program=Program(()), debug=True)
+        assert engine.debug is True
+
+        engine2 = Engine(program=Program(()), debug=False)
+        assert engine2.debug is False
+
+    def test_engine_creates_metrics_when_debug_true(self):
+        """Engine creates EngineMetrics when debug=True."""
+        engine = Engine(program=Program(()), debug=True)
+        assert hasattr(engine, 'metrics')
+        assert isinstance(engine.metrics, EngineMetrics)
+
+    def test_engine_no_metrics_when_debug_false(self):
+        """Engine doesn't create metrics when debug=False."""
+        engine = Engine(program=Program(()), debug=False)
+        assert not hasattr(engine, 'metrics') or engine.metrics is None
+
+    def test_metrics_track_unifications(self):
+        """Metrics track unification attempts/successes during execution."""
+        engine = engine_for("foo(1). foo(2).")
+
+        # Query that will attempt unifications
+        results = engine.query("foo(X)")
+
+        # Should have attempted some unifications
+        assert engine.metrics.unifications_attempted > 0
+        # Some should have succeeded (at least 2 for the solutions)
+        assert engine.metrics.unifications_succeeded >= 2
+
+    def test_metrics_track_backtracks(self):
+        """Metrics track backtracking during execution."""
+        engine = engine_for("test(1). test(2). test(3).")
+
+        # Query for all solutions (will backtrack)
+        results = engine.query("test(X)")
+
+        # Should have backtracked at least twice (after first and second solutions)
+        assert engine.metrics.backtracks_taken >= 2
+
+    def test_metrics_track_cuts(self):
+        """Metrics track cut operations."""
+        # Create a simple program with cut
+        prog = program(
+            mk_rule("test", (Var(0, "X"),),
+                    Struct("=", (Var(0, "X"), Int(1))),
+                    Atom("!")),
+            mk_rule("test", (Var(0, "X"),),
+                    Struct("=", (Var(0, "X"), Int(2))))
+        )
+        engine = Engine(program=prog, debug=True)
+
+        # Query that will execute a cut
+        results = engine.query("test(X)")
+
+        # Should have executed one cut
+        assert engine.metrics.cuts_executed == 1
+        # Should have pruned at least one alternative (the second clause)
+        assert engine.metrics.alternatives_pruned >= 1
+
+    def test_metrics_reset_between_queries(self):
+        """Metrics reset automatically when engine is reset."""
+        engine = engine_for("test(1).")
+
+        # First query
+        engine.query("test(X)")
+        first_unifications = engine.metrics.unifications_attempted
+        assert first_unifications > 0
+
+        # Reset the engine; metrics should reset as part of this
+        engine.reset()
+
+        # Metrics should be back to 0 (no manual metrics.reset() needed)
+        assert engine.metrics.unifications_attempted == 0
+
+        # Second query
+        engine.query("test(X)")
+        # Should have new counts
+        assert engine.metrics.unifications_attempted > 0
+
+    def test_metrics_track_predicate_calls(self):
+        """Metrics track per-predicate call/exit/fail."""
+        engine = engine_for("""
+            parent(tom, bob).
+            parent(bob, pat).
+            grandparent(X, Z) :- parent(X, Y), parent(Y, Z).
+        """)
+
+        # Query for grandparent
+        results = engine.query("grandparent(tom, Z)")
+
+        # Should have metrics for predicates
+        gp_stats = engine.metrics.get_predicate_metrics("grandparent/2")
+        assert gp_stats.calls >= 1
+
+        parent_stats = engine.metrics.get_predicate_metrics("parent/2")
+        assert parent_stats.calls >= 2  # Called at least twice in the rule
+
+    def test_metrics_zero_overhead_when_disabled(self):
+        """No metrics overhead when debug=False."""
+        engine = engine_for("test(1). test(2).", debug=False)
+
+        # Should not have metrics attribute or it's None
+        assert not hasattr(engine, 'metrics') or engine.metrics is None
+
+        # Query should work without metrics
+        results = engine.query("test(X)")
+        assert len(results) == 2
+
+    def test_metrics_export_format(self):
+        """Exported metrics are JSON-compatible."""
+        import json
+
+        engine = engine_for("test(1).")
+        engine.query("test(X)")
+
+        # Export metrics
+        metrics_dict = engine.metrics.to_dict()
+
+        # Should be JSON serializable
+        json_str = json.dumps(metrics_dict)
+        assert isinstance(json_str, str)
+
+        # Can round-trip
+        parsed = json.loads(json_str)
+        assert parsed == metrics_dict
+
+    @pytest.mark.xfail(reason="throw/catch integration not yet wired in engine")
+    def test_metrics_track_exceptions_integration(self):
+        """Metrics track exceptions thrown and caught."""
+        engine = engine_for("""
+            risky(X) :- throw(error(bad(X))).
+            safe(X) :- catch(risky(X), E, true).
+        """)
+
+        engine.query("safe(1)")
+        assert engine.metrics.exceptions_thrown >= 1
+        assert engine.metrics.exceptions_caught >= 1
\ No newline at end of file
